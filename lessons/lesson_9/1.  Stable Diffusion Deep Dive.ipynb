{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1175e02",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af599f26",
   "metadata": {},
   "source": [
    "Stable Diffusion is a text-to-image model, which has been integrated into the Huggingface diffusers library. Examples of text-to-images using stability diffusion models can be found on [Lexica](https://lexica.art). \n",
    "\n",
    "Using the diffusers library on can generate images a simple as:\n",
    "\n",
    "``` python\n",
    "from diffusers import StableDiffusionPipeline\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", revision=\"fp16\", torch_dtype=torch.float16, use_auth_token=True).to(\"cuda\")\n",
    "image = pipe(\"An astronaught scuba diving\").images[0]\n",
    "```\n",
    "\n",
    "In this notebook we're going to dig into the code behind these easy-to-use interfaces, to see what is going on under the hood. \n",
    "\n",
    "We'll begin by re-creating the functionality above as a scary chunk of code, and then one by one we'll inspect the different components and figure out what they do. By the end of this notebook that same sampling loop should feel like something you can tweak and modify as you like."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c03a7f",
   "metadata": {},
   "source": [
    "# Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9ca3d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -q --upgrade transformers diffusers ftfy\n",
    "#!pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "456035a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from base64 import b64encode\n",
    "\n",
    "import numpy\n",
    "import torch\n",
    "from diffusers import AutoencoderKL, LMSDiscreteScheduler, UNet2DConditionModel\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "# For video display:\n",
    "from IPython.display import HTML\n",
    "from matplotlib import pyplot as plt\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from torch import autocast\n",
    "from torchvision import transforms as tfms\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import CLIPTextModel, CLIPTokenizer, logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd67ce6",
   "metadata": {},
   "source": [
    "Before we can download models from HuggingFace, we have to login to HuggingFace using a token. We do this here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d425f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging into Huggingface\n",
    "torch.manual_seed(1)\n",
    "if not (Path.home()/'.huggingface'/'token').exists(): notebook_login()\n",
    "\n",
    "# Supress some unnecessary warnings when loading the CLIPTextModel\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd30ffef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38932a68",
   "metadata": {},
   "source": [
    "# Loading Models\n",
    "Here we load a model using the [Huggingface examples notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion.ipynb).\n",
    "\n",
    "This code will download and set up the relevant models and components we'll be using. Let's just run this for now and move on to the next section to check that it all works before diving deeper.\n",
    "\n",
    "If you've loaded a pipeline, you can also access these components using pipe.unet, pipe.vae and so on.\n",
    "\n",
    "In this notebook we aren't doing any memory-saving tricks - if you find yourself running out of GPU RAM, look at the pipeline code for inspiration with things like attention slicing, switching to half precision (fp16), keeping the VAE on the CPU and other modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3ca9a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \n",
      "```\n",
      "pip install accelerate\n",
      "```\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "# Load the autoencoder model which will be used to decode the latents into image space. \n",
    "vae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a5185a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer and text encoder to tokenize and encode the text. \n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21919951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The UNet model for generating the latents.\n",
    "unet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27378e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The noise scheduler\n",
    "scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da80d4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To the GPU we go!\n",
    "vae = vae.to(torch_device)\n",
    "text_encoder = text_encoder.to(torch_device)\n",
    "unet = unet.to(torch_device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f74f4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
